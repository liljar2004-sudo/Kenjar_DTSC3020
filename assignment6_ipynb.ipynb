{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOkBLNHmLN9SlBBTOD45WH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liljar2004-sudo/Kenjar_DTSC3020/blob/main/assignment6_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Q1 Skeleton (filled) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML, flatten headers.\"\"\"\n",
        "    # Read all tables from HTML\n",
        "    tables = pd.read_html(html)\n",
        "\n",
        "    # Pick the first table with >= 3 columns\n",
        "    for df in tables:\n",
        "        if df.shape[1] >= 3:\n",
        "            # Flatten MultiIndex columns if any\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                df.columns = ['_'.join([str(i) for i in col if i]) for col in df.columns]\n",
        "            return df\n",
        "    raise ValueError(\"No table with >= 3 columns found\")\n",
        "\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\"\"\"\n",
        "    # Make column names consistent\n",
        "    df = df.rename(columns=lambda x: x.strip())\n",
        "\n",
        "    # Pick expected columns (or rename them if necessary)\n",
        "    expected_cols = ['Country', 'Alpha-2', 'Alpha-3', 'Numeric']\n",
        "    df = df.loc[:, df.columns.intersection(expected_cols)]\n",
        "\n",
        "    # Strip strings\n",
        "    for col in ['Country', 'Alpha-2', 'Alpha-3']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip().str.upper()\n",
        "\n",
        "    # Convert Numeric to nullable int\n",
        "    if 'Numeric' in df.columns:\n",
        "        df['Numeric'] = pd.to_numeric(df['Numeric'], errors='coerce').astype('Int64')\n",
        "\n",
        "    # Drop rows with missing mandatory fields\n",
        "    df = df.dropna(subset=['Country', 'Alpha-2', 'Alpha-3', 'Numeric'])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\"\"\"\n",
        "    return df.sort_values(by='Numeric', ascending=False).head(top)\n",
        "\n",
        "\n",
        "# --- Example usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    html_file = \"https://www.iana.org/assignments/language-subtag-registry/language-subtag-registry.html\"\n",
        "    df_raw = q1_read_table(html_file)\n",
        "    df_clean = q1_clean(df_raw)\n",
        "    df_top15 = q1_sort_top(df_clean, top=15)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_clean.to_csv(\"data_q1.csv\", index=False)\n",
        "\n",
        "    # Print Top-15\n",
        "    print(df_top15)\n"
      ],
      "metadata": {
        "id": "gl53aoYfNZBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# --- Q2 Skeleton (filled) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns: rank, title, link, points, comments, user (optional).\"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    rows = soup.select(\".athing\")\n",
        "\n",
        "    items = []\n",
        "    for row in rows:\n",
        "        rank = row.select_one(\".rank\").text if row.select_one(\".rank\") else \"\"\n",
        "        title_tag = row.select_one(\".storylink\")\n",
        "        title = title_tag.text if title_tag else \"\"\n",
        "        link = title_tag.get(\"href\") if title_tag else \"\"\n",
        "\n",
        "        subtext_row = row.find_next_sibling(\"tr\")\n",
        "        subtext = subtext_row.select_one(\".subtext\") if subtext_row else None\n",
        "\n",
        "        points = 0\n",
        "        comments = 0\n",
        "        user = \"\"\n",
        "        if subtext:\n",
        "            # Extract points\n",
        "            points_tag = subtext.select_one(\".score\")\n",
        "            if points_tag:\n",
        "                points_match = re.search(r\"(\\d+)\", points_tag.text)\n",
        "                points = int(points_match.group(1)) if points_match else 0\n",
        "            # Extract comments\n",
        "            comments_tag = subtext.find_all(\"a\")[-1] if subtext.find_all(\"a\") else None\n",
        "            if comments_tag and \"comment\" in comments_tag.text:\n",
        "                comments_match = re.search(r\"(\\d+)\", comments_tag.text)\n",
        "                comments = int(comments_match.group(1)) if comments_match else 0\n",
        "            # Extract user\n",
        "            user_tag = subtext.select_one(\".hnuser\")\n",
        "            user = user_tag.text if user_tag else \"\"\n",
        "\n",
        "        items.append({\n",
        "            \"rank\": rank,\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"points\": points,\n",
        "            \"comments\": comments,\n",
        "            \"user\": user\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\"\"\"\n",
        "    # Clean rank, points, comments\n",
        "    for col in ['rank', 'points', 'comments']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Fill missing text fields\n",
        "    for col in ['title', 'link', 'user']:\n",
        "        df[col] = df[col].fillna(\"\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points descending and return Top-N.\"\"\"\n",
        "    return df.sort_values(by='points', ascending=False).head(top)\n",
        "\n",
        "\n",
        "# --- Example usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://news.ycombinator.com/\"\n",
        "    html = requests.get(url).text\n",
        "\n",
        "    df_raw = q2_parse_items(html)\n",
        "    df_clean = q2_clean(df_raw)\n",
        "    df_top15 = q2_sort_top(df_clean, top=15)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_clean.to_csv(\"data_q2.csv\", index=False)\n",
        "\n",
        "    # Print Top-15\n",
        "    print(df_top15)\n"
      ],
      "metadata": {
        "id": "1fInuGvbNzvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}